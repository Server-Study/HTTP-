# 웹 로봇

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
* 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 주가 추이 그래프를 생성하는 주식 그래프 로봇
* 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇
    * 웹을 떠돌면서 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록
    * http://www.netcraft.com 는 웹 전체에서 사이트들이 어떤 종류의 서버를 사용하고 있는지에 대한 방대한 통계 자료를 수집
* 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
* 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹페이지를 수집하는 가격 비교 로봇

### 크롤러와 크롤링
웹 크롤러는, 먼저 웹 페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹 페이지들을 가져오는 일을 재귀적으로 반복하는 웹 순회 로봇
* 인터넷 검색엔진은 크롤러를 이용해서 가져온 문서들을 검색 가능한 데이터베이스로 만든다.
* 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

#### 어디에서 시작하는가: ‘루트 집합’
크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합이라고 불린다.
* 모든 링크를 크롤링하면 결과적으로 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL들을 선택해야 한다.
* 몇몇 웹페이지들을 그들로 향하는 어떤 링크도 없이 오도 가도 못하게 거의 고립되어 있다.
* 일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 페이지들의 목록으로 구성

#### 링크 추출과 상대 링크 정상화
* 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가 해야 한다.
* 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라, 이 목록은 보통 급속히 확장된다.
* 크롤러는 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다.

#### 순환 피하기
로봇이 웹을 크롤링 할 때, 루프에 순환에 빠지지 않도록 매우 조심해야 한다.
* 순환을 피하기 이해 그들이 어디를 방문했는지 알아야 한다.

#### 루프와 중복
순환은 최소 다음의 세 이유로 인해 크롤러에게 해롭다.
* 루프에 빠뜨려서 꼼짝 못하게 만들 수 있다.
* 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다.
    * 이러한 서비스 방해 행위는 법적인 문제제기의 근거가 될 수도 있다.
* 크롤러의 애플리케이션은 자신을 쓸모없게 만드는 중복된 콘텐츠로 넘쳐나게 될 것이다.

#### 빵 부스러기 흔적
불행히도, 방문한 곳을 지속적으로 추적하는 것은 쉽지 않다.
* URL들은 굉장히 많기 때문에, 어떤 URL을 방문했는지 빠르게 판단하기 위해서는 복잡한 자료 구조를 사용할 필요가 있다.
* 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것이다.

트리와 해시 테이블
* 이들은 URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료구조

느슨한 존재 비트맵
* 공간 사용을 최소화 하기 위해, 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용
* 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 존재 비트를 갖는다.
* 만약 존재 비트가 존재하면, 이미 크롤링 되었다고 간주

체크 포인트
* 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인

파티셔닝
* 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다.
* 대규모 웹 로봇은, 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장을 이용

#### 별칭(alias)과 로봇 순환
올바른 자료구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주는 게 쉽지 않을 때도 있다.

#### URL 정규화하기
URL들을 표준 형식으로 ‘정규화’ 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다.
* 포트 번호가 명시되지 않았다면, 호스트 명에 ‘:80’을 추가
* 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환
* '#' 태그들을 제거

각 웹 서버에 대한 지식 없이 중복을 피할수 있는 좋은 방법은 없다.
* 대소문자를 구분하는지
* URL들이 같은 리소스를 가리키는지 알려면 웹 서버의 색인 페이지 설정을 알 필요가 있다.
* 웹 서버가 가상 호스팅을 하도록 설정되어 있는지

#### 파일 시스템 링크 순환
파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교모한 종류의 순환을 유발할 수 있다.
* 서버 관리자가의 실수가 보통이지만, 악의적으로 만들 수도 있다.

#### 동적 가상 웹 공간
크롤러를 함정으로 빠뜨리기 위해 의도적으로 복잡한 크롤러 루프를 만드는 것은 있을 수 있는 일이다.

심벌릭 링크나 동적 콘텐츠를 통한 크롤러를 함정에 빠트릴 수 있다.
* 달력을 생성하고 다음 달로 링크를 걸어주는 CGI 프로그램이 있다면 로봇은 무한히 다음 달 달력을 요청할 수 있다.

#### 루프와 중복 피하기
모든 순환을 피하는 완벽한 방법은 없다. 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
* 더욱 자율적인 크롤러는 더 쉽게 곤란한 상황에 부딪힌다.
    * 휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 ‘손실’을 유발할 수도 있다.

웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법
* URL 정규화
* 너비 우선 크롤링
    * 함정을 건드리게 되더라도, 여전히 그 순환에서 페이지를 받아오기 전에 다른 웹 사이트들에서 수십만 개의 페이지들을 받아올 수 있다.
* 스로틀링
    * 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
* URL 크기 제한
* URL/사이트 블랙리스트
* 패턴 발견
    * 몇몇 로봇은 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고, 둘 혹은 셋 이상의 반복된 구성요소를 갖고 있는 URL을 크롤링하는 것을 거절
* 콘텐츠 지문(fingerprint)
    * 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다.
    * 지문 생성용으로 MD5와 같은 메시지 요약 함수가 인기 있다.
    * 동적으로 바뀔 수 있는 링크나 임의로 커스터마이징(날짜 추가, 카운터 접근 등)하는 부분들은 체크섬 계산에서 빠뜨린다.
* 사람의 모니터링

### 로봇의 HTTP
로봇들은 다른 HTTP 클라 프로그램과 다르지 않다.
* 그들 또한 HTTP 명세의 규칙을 지켜야 한다.
* 많은 로봇이 필요한 HTTP를 최소한으로 구현하려 하고, 요구사항이 적은 HTTP/1.0 요청을 보낸다.

#### 요청 헤더 식별하기
로봇들이 HTTP를 최소한도로만 지원하려고 함에도 불구하고, 약간의 신원 식별 헤더를 구현하고 전송한다.
* User-Agent
    * 요청을 만든 로봇의 이름
* From
    * 로봇의 사용자/관리자의 이메일 주소
* Accept
    * 어떤 미디어 타입을 보내도 되는지 말해준다.
    * 관심 있는 유형의 콘텐츠만 받게 될 것임을 확신하는데 도움을 준다.
* Referer
    * 현재의 요청 URL을 포함한 문서의 URL을 제공
    * 어떻게 로봇이 그들 사이트의 콘텐츠에 대한 링크를 발견했는지 알아내고 싶은 사이트 관리자들에게 매우 유용

#### 가상 호스팅
Host 헤더를 지원할 필요가 있다.
* 가상 호스팅이 널리 펴져있는 현실에서, 요청에 Host 헤더를 포함하지 않으면 URL에 대해 잘못된 컨텐츠를 찾게 만든다.
* HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.

#### 조건부 요청
때때로 로봇들이 극악한 양의 요청을 시도한다는 것을 고려할 때, 로봇이 검색하는 콘텐츠의 양을 최소화하는 것은 상당히 의미 있는 일이다.
* 오직 변경되었을 때만 콘텐츠를 가져오도록
* 시간이나 엔티티 태그를 비교함으로써 조건부 요청을 구현

#### 응답 다루기
주 관심사가 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에, 응답 다루기라고 부를 만한 일은 거의 하지 않는다.

상태 코드
* 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.
    * 200 OK나 404 Not Found
* 명시적으로 이해할 수 없는 상태 코드는, 그 상태 코드가 속한 분류에 근거하여 다루어야 한다.
* 모든 서버가 언제나 항상 적절한 에러코드를 반환하지는 않는다. 200 OK로 에러 기술하여 응답하기도 한다.

엔티티
HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔티티 자체의 정보를 찾을 수 있다.
* 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보
* http-equiv 태그 자체는 콘텐츠 저자가 콘텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단
* ```<meta http-equiv=“Refresh” content=“1; URL=index.html”>```
    * HTTP 응답 값이 “1; URL=index.html”인 Refresh HTTP 헤더를 포함하고 있는 것처럼 다루게 한다.

#### User-Agent 타기팅
많은 웹 사이트들은 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화한다.
* 이렇게 함으로써, 사이트는 로봇에게 콘텐츠 대신 에러 페이지를 제공
* 풍부한 기능을 갖추지 못한 브라우저나 로봇 등 다양한 클라에 잘 대응하는 유연한 페이지를 개발하는 전략을 세워야 한다.

### 부적절하게 동작하는 로봇들
제멋대로인 로봇들이 아수라장을 만들 여러 가능성이 있다.

폭주하는 로봇
* 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있다.
* 흔히 빠른 네트워크 연결을 갖춘 빠른 컴퓨터 위에서 동작
* 에러를 갖거나 순환에 빠지면 웹 서버에 극심한 부하를 안겨준다.
* 로봇 저자들은 폭주 방지를 위한 보호 장치를반드시 극히 신경 써야 한다.

오래된 URL
* 몇몇 로봇은 오래된 URL 목록을 방문한다.
* 웹 사이트가 그들의 콘텐츠를 많이 바꾸었다면, 존재하지 않는 URL에 대한 요청을 많이 보낸다.
* 존재하지 않는 문서 접근에 대한 에러 로그가 채워지거나, 에러 페이지를 제공하는 부하로 인해 웹 서버의 요청 수용 능력이 감소

호기심이 지나친 로봇
* 어떤 로봇은 사적인 데이터에 대한 URL을 얻어 그 데이터를 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수 있다.
    * 사생활 침해가 될 수 있다.
* 매우 광적인 로봇이 명시적으로는 하이퍼링크가 존재하지도 않는 문서들을 디렉터리의 콘텐츠를 가져오는 등의 방법으로 긁어올 때 가끔 일어난다.
* 민감한 데이터를 무시하는 메커니즘은 명백히 중요

동적 게이트웨이 접근
* 로봇들이 그들이 접근하고 있는 것에 대해 언제나 잘 알고 있는 것은 아니다.
* 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청할 수 있는데 이 경우 얻은 데이터는 아마도 특수 목적을 위한 것이고 처리 비용도 많이 든다.

### 로봇 차단하기
로봇 커뮤니티는 로봇에 의한 웹 사이트 접근이 유발할 수 있는 문제를 알고 있었다.
* 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안
* 로봇의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 종종 그냥 robots.txt라고 불린다.

robots.txt의 아이디어
* 어떤 웹 서버는 문서 루트에 robots.txt라고 이름 붙은 선택적인 파일을 제공
    * 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨 있다.
* 어떤 로봇이 자발적인 표준에 따른다면, 다른 리소스에 접근하기 전에 우선 그 사이트의 robots.txt를 요청

#### 로봇 차단 표준
로봇 차단 표준은 임시방편으로 마련된 표준이다.
* 여전히 로봇의 접근을 제어하는 능력은 불완전한 데가 있지만 없는 것보단 낫고 주류 업체들과 검색엔진 크롤러들이 이 차단 표준을 지원

#### 웹 사이트와 robots.txt 파일들
웹 사이트의 어떤 URL을 방문하기 전에, robots.txt 파일이 존재한다면 반드시 그 파일을 가져와서 처리해야 한다.
* 반드시 파일 시스템에 있을 필요는 없고, 예를 들어 게이트웨이 애플리케이션이 동적으로 생성할 수도 있다.
* 호스트 명과 포트번호에 의해 정의되는 사이트 전체에 대한 robots.txt 파일은 단 하나만이 존재. 
* 만약 가상 호스팅 된다면, 각각의 가상 docroot에 서로 다른 robots.txt가 존재

robots.txt 가져오기
* HTTP GET 메서드를 이용해 robots.txt를 가져온다.
* 404 Not Found 상태 코드면 서버는 로봇의 접근을 제한하지 않는 것으로 간주
* 로봇은 From이나 User-Agent 헤더를 통해 신원 정보를 넘기고, 사이트 관리자가 불만사항이 있을 경우를 위해 연락처를 제공해야 한다.

응답 코드
* 로봇은 어떤 웹 사이트든 반드시 robots.txt를 찾아본다.
* 서버가 성공(2XX)하면 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고, 규칙을 따라야 한다.
* 404 상태코드면 활성화된 차단 규칙이 존재하지 않는다고 가정
* 접근 제한(401 혹은 403)으로 응답한다면 그 사이트로의 접근은 완전히 제한되었다고 가정
* 일시적으로 실패(503) 했다면 그 사이트의 리소스를 검색하는 것은 뒤로 미루어야 한다.
* 서버 응답이 리다이렉션을 의미(3XX)한다면 리소스가 발견될 때까지 리다이렉트를 따라가야 한다.

#### robots.txt 파일 포맷
매우 단순한 줄 기반 문법
* 각 줄은 빈줄, 주석 줄, 규칙 줄의 세 가지 종류가 있다.

```
# robots.txt 파일

User-Agent: slurp
Disallow: /private

User-Agent: *
Disallow:
```

User-Agent 줄
* 로봇의 이름은 GET 요청 안의 User-Agent 헤더를 통해 보내진다.
* robots.txt를 처리한 로봇은 다음의 레코드에 반드시 복종해야 한다.
    * 로봇 이름이 자신 이름의 부분 문자열이 될 수 있는 레코드들 중 첫 번째 것.
    * 로봇 이름이 ‘*’인 레코드들 중 첫 번째 것
* 로봇 이름을 대소문자를 구분하지 않는 부분 문자열과 맞춰보므로, 의도치 않게 맞는 경우에 주의해야 한다.

Disallow와 Allow 충돌
* 로봇은 반드시 요청하려고 하는 URL을 차단 레코드의 모든 규칙에 순서대로 맞춰 보아야 한다.
    * 첫 번째로 맞는 것이 사용
* 규칙 경로는 반드시 맞춰보고자 하는 경로의 대소문자를 구분하는 접두어여야 한다.

Disallow/Allow 접두 매칭(prefix matching)
* 그 경로의 시작부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 같아야 한다.
    * 대소문자 차이도 없어야 한다.
* User-Agent와 달리 별표(*)는 특별한 의미를 갖지 않지만, 대신 빈 문자열을 이용해 모든 문자열에 매치되도록 할 수 있다.
* ‘이스케이핑’된 문자들은 비교 전에 원래대로 복원된다.
    * 빗금(/)을 의미하는 %2F는 예외

#### 그 외 알아둘 점
robots.txt 파일을 파싱할 때 지켜야 할 규칙이 몇 가지 더 있다.
* 다른 여러 필드를 포함할 수 있다.
    * 모르는 필드는 무시해야 한다.
* 하위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않는다.
* 주석은 파일의 어디에서든 허용
    * 선택적인 공백 문자와 뒤이은 주석 문자(#)로 시작
* 0.0 버전은 Allow 줄을 지원하지 않는다.

#### robots.txt의 캐싱과 만료
로봇은 주기적으로 robots.txt를 가져와서 그 결과를 캐시해야 한다.
* 표준 HTTP 캐시 제어 메커니즘이 원 서버와 로봇 양쪽 모두에 의해 사용
* 웹 마스터들은 크롤러들이 robots.txt 리소스에 캐시 지시자를 이해하지 못할 수도 있는 점을 주의

#### HTML 로봇 제어 META 태그
robots.txt 파일의 단점 중 하나는 그 파일을 콘텐츠의 작성자 개개인이 아니라 웹 사이트 관리자가 소유
* HTML 문서에 직접 로봇 제어 태그를 추가해 개별 페이지에 접근을 제한하는 직접적인 방법을 갖고 있다.
* 로봇은 여전히 문서를 가져올 수는 있겠지만, 로봇 차단 태그가 존재한다면 그 문서를 무시
* ```<META NAME=“ROBOTS” CONTENT=directive-list>```

로봇 MEATA 지시자
* 로봇 META 지시자에는 몇 가지 종류가 있으며, 시간이 지나면서 새 지시자가 추가될 가능성이 높다.
    * NOINDEX
        * 이 페이지를 처리하지 말고 무시해라(콘텐츠를 색인이나 DB에 포함시키지 말 것)
    * NOFOLLOW
        * 이 페이지가 링크한 페이지를 크롤링하지 마라
    * INDEX
        * 이 페이지의 콘텐츠를 인덱싱해도 된다.
    * FOLLOW
        * 이 페이지가 링크한 페이지를 크롤링해도 된다.
    * NOARCHIVE
        * 이 페이지의 캐시를 위한 로컬 사본을 만들어서는 안된다.
    * ALL
        * INDEX, FOLLOW와 같다.
    * NONE
        * NOIDEX, NOFOLLOW와 같다.
* 다른 모든 HTML META 태그와 마찬가지로 반드시 HTML 페이지의 HEAD 섹션에 나타나야 한다.
* name과 content의 값은 대소문자를 구분하지 않는다.
* 지시들이 서로 충돌하거나 중복되게 해서는 안된다.

검색엔진 META 태그
* 다른 많은 META 태그들이 있다.
    * DESCRIPTION
        * 웹 페이지의 짧은 요약을 정의
    * KEYWORDS
        * 키워드 검색을 돕기 위한, 웹페이지를 기술하는 단어들의 쉼표로 구분되는 목록
    * REVISIT_AFTER
        * 지정한 만큼의 날짜가 지난 이후에 다시 방문해야 한다고 지시

### 로봇 에티켓
웹 로봇 커뮤니티의 개척자인 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인을 작성했다.
* 몇 가지는 구식이 되어버렸지만, 대다수는 아직도 상당히 유용
* 표 9-6

### 검색엔진
웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진
* 오늘날 가장 유명한 웹 사이트들의 상당수가 검색엔진
* 웹 크롤러들은 마치 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다 준다.
    * 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 해준다.

#### 넓게 생각하라
웹의 초창기때는 검색엔진들은 웹상에서 문서의 위치를 알아내는 단순한 데이터베이스였다. 웹에서 수십억 개의 페이지들이 접근 가능한 오늘날, 웹은 진화하면서 꽤나 복잡해졌다.
* 오늘날 검색엔진은 수십억 개의 웹페이지들을 검색하기 위해 복잡한 크롤러를 사용해야 한다.
* 대규모 크롤러가 자신의 작업을 완료하려면 많은 장비를 똑똑하게 사용해서 요청을 병렬로 수행할 수 있어야 할 것이다.

#### 현대적인 검색엔진의 아키텍처
오늘날 검색엔진들은 그들이 갖고 있는 전 세계의 웹페이지들에 대해 ‘풀 텍스트 색인(full-text indexes)’라고 하는 복잡한 로컬 데이터베이스를 생성한다.
* 이 색인은 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작
* 크롤링은 한 번 하는데 걸리는 시간이 상당한데 비해 웹페이지들은 매 순간 변화하기 때문에, 풀 텍스트 색인은 기껏 해봐야 웹의 특정 순간에 대한 스냅숏에 불과하다.

#### 플 텍스트 색인
단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스
* 이 문서들은 색인이 생성된 후에는 검색할 필요가 없다.

#### 질의 보내기
사용자가 질의를 웹 검색엔진으로 보내는 방법은, HTML 폼을 사용자가 채워 넣고 브라우저가 GET이나 POST 요청을 이용해서 게이트웨이로 보내는 식이다.

#### 검색 결과를 정렬하고 보여주기
질의의 결과를 확인하기 위해 검색엔진이 색인을 한번 사용했다면, 게이트웨이 애플리케이션은 그 결과를 이용해 최종 사용자를 위한 결과 페이지를 즉석에서 만들어낸다.
* 검색엔진은 결과에 순위를 매기기 위해 똑똑한 알고리즘을 사용한다.
* 관련이 많은 순서대로 결과 문서에 나타날 수 있도록 관련도 랭킹(relevancy ranking)이라 불리는 점수를 매기고 정렬하는 과정을 한다.
    * 예를 들어, 어떤 주어진 페이지를 가리키는 링크들이 얼마나 많은지 세는 것은 그 문서의 인기도를 판별하는데 도움이 된다.
* 검색엔진에 의해 사용되는 알고리즘, 크롤링에 대한 팁, 그 외 각종 기교는 검색엔진의 가장 엄격히 감추어진 비밀들이다.

#### 스푸핑
* 사용자에게는 웹 사이트를 찾을 때 검색 결과의 순서는 중요하다.
* 웹 마스터에게는 자신이 만든 사이트가 검색 결과의 상단에 노출되도록 만들 동기가 충분하다.
* 검색 결과에서 더 높은 순위를 차지하고자 하는 바람은 검색 시스템과의 게임으로 이어져서 끝나지 않는 줄다리기를 만들어냈다.
    * 웹 마스터가 수많은 키워드들을 나열한 가짜 페이지를 만들거나
    * 검색 엔진의 관련도 알고리즘을 더 잘 속일 수 있는, 특정 단어에 대한 가짜 페이지를 생성하는 게이트웨이 애플리케이션을 만들어 사용한다.
* 검색엔진과 로봇 구현자들은 이러한 속임수를 더 잘 잡아내기 위해 끊임없이 그들의 관련도 알고리즘을 수정해야만 한다.